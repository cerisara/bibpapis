abstract: Continual learning in task-oriented dialogue systems can allow us to add
  new domains and functionalities through time without incurring the high cost of
  a whole system retraining. In this paper, we propose a continual learning benchmark
  for task-oriented dialogue systems with 37 domains to be learned continuously in
  four settings, such as intent recognition, state tracking, natural language generation,
  and end-to-end. Moreover, we implement and compare multiple existing continual learning
  baselines, and we propose a simple yet effective architectural method based on residual
  adapters. Our experiments demonstrate that the proposed architectural method and
  a simple replay-based strategy perform comparably well but they both achieve inferior
  performance to the multi-task learning baseline, in where all the data are shown
  at once, showing that continual learning in task-oriented dialogue systems is a
  challenging task. Furthermore, we reveal several trade-offs between different continual
  learning methods in term of parameter usage and memory size, which are important
  in the design of a task-oriented dialogue system. The proposed benchmark is released
  together with several baselines to promote more research in this direction.
archiveprefix: arXiv
author: Madotto, Andrea and Lin, Zhaojiang and Zhou, Zhenpeng and Moon, Seungwhan
  and Crook, Paul and Liu, Bing and Yu, Zhou and Cho, Eunjoon and Wang, Zhiguang
author_list:
- family: Madotto
  given: Andrea
- family: Lin
  given: Zhaojiang
- family: Zhou
  given: Zhenpeng
- family: Moon
  given: Seungwhan
- family: Crook
  given: Paul
- family: Liu
  given: Bing
- family: Yu
  given: Zhou
- family: Cho
  given: Eunjoon
- family: Wang
  given: Zhiguang
eprint: 2012.15504v1
file: 2012.15504v1.pdf
files:
- tmp60dx60-4.pdf
month: Dec
primaryclass: cs.CL
ref: 2012.15504v1
time-added: 2022-05-23-10:08:25
title: Continual Learning in Task-Oriented Dialogue Systems
type: article
url: http://arxiv.org/abs/2012.15504v1
year: '2020'
