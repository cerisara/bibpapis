abstract: We explore how generating a chain of thought -- a series of intermediate
  reasoning steps -- significantly improves the ability of large language models to
  perform complex reasoning. In particular, we show how such reasoning abilities emerge
  naturally in sufficiently large language models via a simple method called chain
  of thought prompting, where a few chain of thought demonstrations are provided as
  exemplars in prompting. Experiments on three large language models show that chain
  of thought prompting improves performance on a range of arithmetic, commonsense,
  and symbolic reasoning tasks. The empirical gains can be striking. For instance,
  prompting a 540B-parameter language model with just eight chain of thought exemplars
  achieves state of the art accuracy on the GSM8K benchmark of math word problems,
  surpassing even finetuned GPT-3 with a verifier.
archiveprefix: arXiv
author: Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter,
  Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny
author_list:
- family: Wei
  given: Jason
- family: Wang
  given: Xuezhi
- family: Schuurmans
  given: Dale
- family: Bosma
  given: Maarten
- family: Ichter
  given: Brian
- family: Xia
  given: Fei
- family: Chi
  given: Ed
- family: Le
  given: Quoc
- family: Zhou
  given: Denny
eprint: 2201.11903v4
file: 2201.11903v4.pdf
files:
- tmp1000s6sb.pdf
month: Jan
primaryclass: cs.CL
ref: 2201.11903v4
time-added: 2022-06-21-13:54:50
title: Chain of Thought Prompting Elicits Reasoning in Large Language Models
type: article
url: http://arxiv.org/abs/2201.11903v4
year: '2022'
