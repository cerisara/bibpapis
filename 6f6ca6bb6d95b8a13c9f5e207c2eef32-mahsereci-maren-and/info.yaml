abstract: Early stopping is a widely used technique to prevent poor generalization
  performance when training an over-expressive model by means of gradient-based optimization.
  To find a good point to halt the optimizer, a common practice is to split the dataset
  into a training and a smaller validation set to obtain an ongoing estimate of the
  generalization performance. We propose a novel early stopping criterion based on
  fast-to-compute local statistics of the computed gradients and entirely removes
  the need for a held-out validation set. Our experiments show that this is a viable
  approach in the setting of least-squares and logistic regression, as well as neural
  networks.
archiveprefix: arXiv
author: Mahsereci, Maren and Balles, Lukas and Lassner, Christoph and Hennig, Philipp
author_list:
- family: Mahsereci
  given: Maren
- family: Balles
  given: Lukas
- family: Lassner
  given: Christoph
- family: Hennig
  given: Philipp
eprint: 1703.09580v3
file: 1703.09580v3.pdf
files:
- tmp6uq1bzxj.pdf
month: Mar
primaryclass: cs.LG
ref: 1703.09580v3
time-added: 2022-03-29-15:19:00
title: Early Stopping without a Validation Set
type: article
url: http://arxiv.org/abs/1703.09580v3
year: '2017'
