abstract: Large foundation models can exhibit unique capabilities depending on the
  domain of data they are trained on. While these domains are generic, they may only
  barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale
  image captions, but large language models (LMs) are further trained on Internet-scale
  text with no images (e.g. from spreadsheets, to SAT questions). As a result, these
  models store different forms of commonsense knowledge across different domains.
  In this work, we show that this model diversity is symbiotic, and can be leveraged
  to build AI systems with structured Socratic dialogue -- in which new multimodal
  tasks are formulated as a guided language-based exchange between different pre-existing
  foundation models, without additional finetuning. In the context of egocentric perception,
  we present a case study of Socratic Models (SMs) that can provide meaningful results
  for complex tasks such as generating free-form answers to contextual questions about
  egocentric video, by formulating video Q&A as short story Q&A, i.e. summarizing
  the video into a short story, then answering questions about it. Additionally, SMs
  can generate captions for Internet images, and are competitive with state-of-the-art
  on zero-shot video-to-text retrieval with 42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate
  how to compose foundation models zero-shot to capture new multimodal functionalities,
  without domain-specific data collection. Prototypes are available at socraticmodels.github.io.
archiveprefix: arXiv
author: Zeng, Andy and Wong, Adrian and Welker, Stefan and Choromanski, Krzysztof
  and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas
  and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete
author_list:
- family: Zeng
  given: Andy
- family: Wong
  given: Adrian
- family: Welker
  given: Stefan
- family: Choromanski
  given: Krzysztof
- family: Tombari
  given: Federico
- family: Purohit
  given: Aveek
- family: Ryoo
  given: Michael
- family: Sindhwani
  given: Vikas
- family: Lee
  given: Johnny
- family: Vanhoucke
  given: Vincent
- family: Florence
  given: Pete
eprint: 2204.00598v1
file: 2204.00598v1.pdf
files:
- tmpje3n54k6.pdf
month: Apr
primaryclass: cs.CV
ref: 2204.00598v1
time-added: 2022-04-09-11:10:27
title: 'Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language'
type: article
url: http://arxiv.org/abs/2204.00598v1
year: '2022'
