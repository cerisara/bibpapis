abstract: We show that unconverged stochastic gradient descent can be interpreted
  as sampling from a nonparametric approximate posterior distribution. This distribution
  is implicitly defined by the transformation of an initial distribution by a sequence
  of optimization steps.  By tracking the change in entropy of this distribution during
  optimization, we give a scalable, unbiased estimate of a variational lower bound
  on the log marginal likelihood. This bound can be used to optimize hyperparameters
  instead of cross-validation. This Bayesian interpretation of SGD also suggests new
  overfitting-resistant optimization procedures, and gives a theoretical foundation
  for early stopping and ensembling. We investigate the properties of this marginal
  likelihood estimator on neural network models.
address: Cadiz, Spain
author: Duvenaud, David and Maclaurin, Dougal and Adams, Ryan
author_list:
- family: Duvenaud
  given: David
- family: Maclaurin
  given: Dougal
- family: Adams
  given: Ryan
booktitle: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
editor: Gretton, Arthur and Robert, Christian C.
files:
- pmlr-v51-duvenaud16.bib
month: 09--11 May
pages: 1070--1077
pdf: http://proceedings.mlr.press/v51/duvenaud16.pdf
publisher: PMLR
ref: pmlr-v51-duvenaud16
series: Proceedings of Machine Learning Research
time-added: 2022-04-06-08:10:36
title: Early Stopping as Nonparametric Variational Inference
type: inproceedings
url: https://proceedings.mlr.press/v51/duvenaud16.html
volume: '51'
year: '2016'
