abstract: Transformer models have obtained remarkable accomplishments in various NLP
  tasks. However, these models have efficiency issues on long sequences, as the complexity
  of their self-attention module scales quadratically with the sequence length. To
  remedy the limitation, we present Memformer, a novel language model that utilizes
  a single unified memory to encode and retrieve past information. It includes a new
  optimization scheme, Memory Replay Back-Propagation, which promotes long-range back-propagation
  through time with a significantly reduced memory requirement. Memformer achieves
  $\mathcal{O}(n)$ time complexity and $\mathcal{O}(1)$ space complexity in processing
  long sequences, meaning that the model can handle an infinite length sequence during
  inference. Our model is also compatible with other self-supervised tasks to further
  improve the performance on language modeling. Experimental results show that Memformer
  outperforms the previous long-range sequence models on WikiText-103, including Transformer-XL
  and compressive Transformer.
archiveprefix: arXiv
author: Wu, Qingyang and Lan, Zhenzhong and Gu, Jing and Yu, Zhou
author_list:
- family: Wu
  given: Qingyang
- family: Lan
  given: Zhenzhong
- family: Gu
  given: Jing
- family: Yu
  given: Zhou
eprint: 2010.06891v1
file: 2010.06891v1.pdf
files:
- tmp98ez7n9l.pdf
month: Oct
primaryclass: cs.CL
ref: 2010.06891v1
time-added: 2022-03-15-07:24:54
title: 'Memformer: The Memory-Augmented Transformer'
type: article
url: http://arxiv.org/abs/2010.06891v1
year: '2020'
