abstract: 'Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality
  of the factual information extracted from Large Language Models (LLMs) depends on
  the prompts used to query them. This inconsistency is problematic because different
  users will query LLMs for the same information using different wording, but should
  receive the same, accurate responses regardless. In this work we aim to address
  this shortcoming by introducing P-Adapters: lightweight models that sit between
  the embedding layer and first attention layer of LLMs. They take LLM embeddings
  as input and output continuous prompts that are used to query the LLM. Additionally,
  we investigate Mixture of Experts (MoE) models that learn a set of continuous prompts
  ("experts") and select one to query the LLM. They require a separate classifier
  trained on human-annotated data to map natural language prompts to the continuous
  ones. P-Adapters perform comparably to the more complex MoE models in extracting
  factual information from BERT and RoBERTa while eliminating the need for additional
  annotations. P-Adapters show between 12-26% absolute improvement in precision and
  36-50% absolute improvement in consistency over a baseline of only using natural
  language queries. Finally, we investigate what makes P-Adapters successful and conclude
  that a significant factor is access to the LLM''s embeddings of the original natural
  language prompt, particularly the subject of the entity pair being queried.'
archiveprefix: arXiv
author: Newman, Benjamin and Choubey, Prafulla Kumar and Rajani, Nazneen
author_list:
- family: Newman
  given: Benjamin
- family: Choubey
  given: Prafulla Kumar
- family: Rajani
  given: Nazneen
eprint: 2110.07280v2
file: 2110.07280v2.pdf
files:
- tmp61tiu7k5.pdf
month: Oct
primaryclass: cs.CL
ref: 2110.07280v2
time-added: 2022-04-20-09:24:41
title: 'P-Adapters: Robustly Extracting Factual Information from Language Models   with
  Diverse Prompts'
type: article
url: http://arxiv.org/abs/2110.07280v2
year: '2021'
