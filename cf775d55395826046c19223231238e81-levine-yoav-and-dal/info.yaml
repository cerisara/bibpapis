abstract: 'Huge pretrained language models (LMs) have demonstrated surprisingly good
  zero-shot capabilities on a wide variety of tasks. This gives rise to the appealing
  vision of a single, versatile model with a wide range of functionalities across
  disparate applications. However, current leading techniques for leveraging a "frozen"
  LM -- i.e., leaving its weights untouched -- still often underperform fine-tuning
  approaches which modify these weights in a task-dependent way. Those, in turn, suffer
  forgetfulness and compromise versatility, suggesting a tradeoff between performance
  and versatility. The main message of this paper is that current frozen-model techniques
  such as prompt tuning are only the tip of the iceberg, and more powerful methods
  for leveraging frozen LMs can do just as well as fine tuning in challenging domains
  without sacrificing the underlying model''s versatility. To demonstrate this, we
  introduce three novel methods for leveraging frozen models: input-dependent prompt
  tuning, frozen readers, and recursive LMs, each of which vastly improves on current
  frozen-model approaches. Indeed, some of our methods even outperform fine-tuning
  approaches in domains currently dominated by the latter. The computational cost
  of each method is higher than that of existing frozen model methods, but still negligible
  relative to a single pass through a huge frozen LM. Each of these methods constitutes
  a meaningful contribution in its own right, but by presenting these contributions
  together we aim to convince the reader of a broader message that goes beyond the
  details of any given method: that frozen models have untapped potential and that
  fine-tuning is often unnecessary.'
archiveprefix: arXiv
author: Levine, Yoav and Dalmedigos, Itay and Ram, Ori and Zeldes, Yoel and Jannai,
  Daniel and Muhlgay, Dor and Osin, Yoni and Lieber, Opher and Lenz, Barak and Shalev-Shwartz,
  Shai and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav
author_list:
- family: Levine
  given: Yoav
- family: Dalmedigos
  given: Itay
- family: Ram
  given: Ori
- family: Zeldes
  given: Yoel
- family: Jannai
  given: Daniel
- family: Muhlgay
  given: Dor
- family: Osin
  given: Yoni
- family: Lieber
  given: Opher
- family: Lenz
  given: Barak
- family: Shalev-Shwartz
  given: Shai
- family: Shashua
  given: Amnon
- family: Leyton-Brown
  given: Kevin
- family: Shoham
  given: Yoav
eprint: 2204.10019v1
file: 2204.10019v1.pdf
files:
- tmpm2gxkl28.pdf
month: Apr
primaryclass: cs.CL
ref: 2204.10019v1
time-added: 2022-04-24-08:02:45
title: Standing on the Shoulders of Giant Frozen Language Models
type: article
url: http://arxiv.org/abs/2204.10019v1
year: '2022'
