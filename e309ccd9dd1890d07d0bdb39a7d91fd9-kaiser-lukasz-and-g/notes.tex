papier de 2017 qui experimente un modele compose de plusieurs parties, chacune plutot specialisee
dans une tache, dont des MoE

A Mixture of Experts (MoE) is a special type of neural network: neurons are connected in many small clusters, and each cluster is only active under special circumstances. Lower layers of the network extract features, and experts are called upon to evaluate those features — for each case, only some of the experts are called upon. Mixtures of Experts have distinct advantages: they can respond to particular circumstances with greater specialization, allowing the network to display a greater variety of behaviors; experts can receive a mixture of stimuli, integrating data from diverse sensors; and when the network is in operation, only a few experts are active — even a huge network needs only a small amount of processing power. As neural networks become more complex, integrating many streams of data, and supplying a greater variety of responses, Mixture of Expert models will dominate. So, it helps to understand how Mixtures of Experts could evolve.

Feature-Based Attention

Mixtures of Experts are already in use for translation tasks. Each little expert-cluster learns to handle a separate part of speech or special grammatical rule. Yet, the Mixture of Experts translators do not currently use an Attention model.

Attention is just a filter that allows only some of the input into the network at a time. By successively moving Attention around, a network can handle inputs of any size. An image-recognition network could receive images of many different sizes, and diligently parse the images piece by piece. A translation network could hop around among sentences, forming relationships between words that are separated by many lines. A Mixture of Experts, moving across its inputs this way, would be able to extract much richer relationships between words, and would be better equipped to translate accurately. Each Expert is called upon when certain features are recognized; those features may appear in disparate regions of the input, and require broad understanding of the input. The Experts would know where to pay attention.

Attention Memory

Mixtures of Experts are the ideal model for a more complex form of memory, as well. Parsing a translation often requires that the network pay attention to a few different areas in the text. Statements that occur in multiple places must be combined into a coherent whole, so that the translation makes sense. A Mixture of Experts must focus its attention on an area, while remembering information from another area. This is achieved by wiring expert-clusters to the network’s past states, similar to the wiring of an LSTM.

LSTMs wire each neuron to its own past, without regard to the past state of its neighbors. Mixtures of Experts, however, would be wired to the past states of the feature-detectors that trigger that expert. This is a ‘higher order’ of memory. A feature might appear, and not trigger an expert-cluster. Yet, the memory of that feature, when combined with a relevant input, might trigger its expert later! The expert would be responding to the combination of the present input and the past feature. This behavior is currently impossible with an LSTM or other recurrent networks. And, it makes all the difference.

The Review Process

If a Mixture of Experts has a system of Attention, then it can hop around in a text, seeking information until it finds something that triggers an expert. This is similar to the way that we review what we’ve read; if we become confused, we skip back to a variety of places, until we find the information that clarifies the text. An LSTM cannot hop around until it finds clarity. If the Attention system is connected to feature-level memories, a Mixture of Experts will review its inputs until it settles on a certainty. It looks around for the information that supports a conclusion, instead of blurting out answers.

This review process can be captured and visualized. When we see what information triggered an expert, we know that the network has ‘based its decision on’ that information. And, because each expert handles a very specific sub-task, we know what kind of problem each expert is solving. This is the best way to ‘peek inside’ the black box of neural networks. “We got this answer, because this expert fired when it saw this information…” is as close as we may get to a network that tells us what it was thinking.

Growth

A Mixture of Experts is also ideal for solving problems that change and grow in complexity. When new information must be learned, most networks need to be completely re-trained, and they lose the lessons that they learned before. A Mixture of Experts can retain its old knowledge, and simply insert new clusters of experts, to be trained on the new information. When this ‘augmented’ Mixture of Experts is trained using back-propagation, all the old neural connections are ‘frozen’ — only the new clusters are allowed to learn. These ‘noobs’ quickly become experts at the new tasks, without losing the old expertise in the rest of the network!

Taken together, these modifications supply a Mixture of Experts with the power to adapt, to digest inputs of any size, and to recall complex features that effect outcomes. I expect that these qualities will be critical for the ‘next wave’ of neural network tasks: Coherent Responses. Currently, neural networks that are trained to generate text are learning only from text. Networks trained to identify images are learning only from images. This ‘cloistered’ training will cease. A neural network that generates realistic text will need to know enough about the real world to make its statements logically and physically coherent, not just grammatically correct. An image recognition network will need to try instantiating its image in 3D, to see if it makes sense. Google has begun to work in this area, with “One Model to Learn Them All”, which learns from sparse pairings of text, images, and sound. They still have a long way to go. And, they are not yet using a Mixture of Experts with Attention, Memory, and Growth. We will soon need it.
